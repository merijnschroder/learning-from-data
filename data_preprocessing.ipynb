{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6qriSlxWyDGIY7j1CTijJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merijnschroder/learning-from-data/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTc_5dP6vbgz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords \n",
        "\", \".join(stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "import string\n",
        "from collections import defaultdict\n",
        "import wordsegment\n",
        "wordsegment.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data\n",
        "def read_file(filepath: str):\n",
        "    df = pd.read_csv(filepath, sep='\\t', encoding='utf-8',names=[\"text\", \"hashtag\"])\n",
        "\n",
        "    hashtag = np.array(df['hashtag'].values)\n",
        "    text = np.array(df['text'].values)"
      ],
      "metadata": {
        "id": "pDX-NpcrvgtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lower the words\n",
        "df[\"text\"] = df[\"text\"].str.lower()"
      ],
      "metadata": {
        "id": "3VQDm4CvwODp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
      ],
      "metadata": {
        "id": "wXvUzs6fvmRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove frequentwords\n",
        "FREQWORDS = set([w for (w, wc) in cnt.most_common(2)])\n",
        "def remove_freqwords(text):\n",
        "    \"\"\"custom function to remove the frequent words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])"
      ],
      "metadata": {
        "id": "v7iEVxKRvphw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove rarewords\n",
        "n_rare_words = 10\n",
        "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "def remove_rarewords(text):\n",
        "    \"\"\"custom function to remove the rare words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n"
      ],
      "metadata": {
        "id": "tQs5Efx4vtlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "eS1P-AVYvuPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove emojis\n",
        "def remove_emojis(text):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', text)"
      ],
      "metadata": {
        "id": "RNzmywF0vw2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E.g. '#LunaticLeft' => 'lunatic left'\n",
        "def segment_hashtag(text):\n",
        "    \n",
        "    for i, text in enumerate(text):\n",
        "        text_tokens = text.split(' ')\n",
        "        for j, t in enumerate(text_tokens):\n",
        "            if t.find('#') == 0:\n",
        "                text_tokens[j] = ' '.join(wordsegment.segment(t))\n",
        "        text[i] = ' '.join(text_tokens)\n",
        "    return text"
      ],
      "metadata": {
        "id": "YJ25wBJGvzf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}